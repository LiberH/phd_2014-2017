%!TEX root = ./main.tex
\section{Experimental results}
\label{sec:results}

  % TODO: (2 pages)
  % - Que veut-on mesurer ?
  % - Définition et justification de notre métrique
  % - Tableaux de résultats et description
  % - Analyse et recul

  We have conducted experiments to measure the reduction of the set of memory
  locations that must be included in the model. Given the current restriction of
  \best{}, we have focused on the registers. To do so, \best\ outputs the
  following information for each program:
  \begin{itemize}
    \item the number of registers used either explicitly or implicitly and the
      number of instructions in the original program ;
    \item the number of registers used either explicitly or implicitly and the
      number of instructions in the sliced program (using the slicing criterion
      defined in Section~\ref{sec:slicing}).
  \end{itemize}
 
  %. For
  %each benchmark we have counted the number $\alpha$ of explicitly and
  %implicitly used registers in the binary executable file. These registers are
  %either from the 32 general purpose registers ($r0$, $r1$, dots, $r32$) or from
  %the 5 dedicated registers ($cr$, $xer$, $lr$, $ctr$, $pc$). Then, we have
  %counted the number $\beta$ of explicitly and implicitly used registers in the
  %slice of the binary executable file according to our specific slicing criteria
  %(see subsection~\ref{subsec:slicing-reduction}). Lastly, we have computed the
  %reduction rate $\gamma = (\alpha - \beta) / \alpha$.

  We used the Mälardalen WCET benchmarks~\cite{Gustafsson:WCET2010:Benchmarks}
  to generate the programs. We had to exclude certain programs to account for the
  current limitation of our tool: program containing floating point arithmetic
  or switch-case statements and recursive programs.

  We used the library generated by the \h\ compiler from a description of a
  PowerPC e200z4 core based on the 32 bits PowerPC instruction set. This
  architecture includes 32 general purpose registers ($r0$, $r1$, \ldots, $r31$)
  and 5 dedicated registers ($cr$, $xer$, $lr$, $ctr$, $pc$). We used two
  different compilers: \gcc\ $5.3.1$ and \cosmic\ $4.3.7$. For a given compiler,
  the generated binary may be very different according to the optimizations. For
  instance without optimization \gcc\ generates code where local variables are
  loaded from and stored to the stack frame each time they are used, whereas in
  higher optimization levels local variables are allocated in registers. Thus we
  created different program versions for each optimization level offered by each
  compiler (4 levels for \gcc\ and 2 levels for \cosmic{}).
  
  All in all, we created 6 versions of each of the 16 Mälardalen benchmarks
  fitting our constraints and we ran \best\ on these 96 programs. Due to space
  limitations the detailed results are provided online\footnote{available at
    \url{https://github.com/TrampolineRTOS/BEST}}. The results are summarized in
  Table~\ref{tab:recap} and~\ref{tab:reginslice}. Table~\ref{tab:recap} gives
  the ratio of registers and instructions in the slice compared to the original
  program. Table~\ref{tab:reginslice} gives the number of registers in the
  slice. It is not meaningful to compare our results with pior work~\cite{CB13}
  because we consider a different instruction set and different compilers (or at
  least compiler version for~\textsc{Gcc}). We do not comment either on the
  execution time of \best\ that were below one second in every case.
  
  \begin{table}
      \centering
      \resizebox{\columnwidth}{!}{%
          \input{fig/recap}%
      }
      \caption{\centering Ratio of registers (resp. instructions) in slice compared to the unsliced program.% \\
      }
      \label{tab:recap}
  \end{table}

  \begin{table}
      \centering
      %\resizebox{.55\columnwidth}{!}{%
      \input{fig/reginslice}%
      %}
      \caption{\centering Average number of registers in slice.% \\
      }
      \label{tab:reginslice}
  \end{table}

  
  These results confirm that slicing is an effective abstraction technique for
  our use case.  It allows a significant reduction of the number of variables
  that should be included in the model (reduction of the dimension of the state
  space) as well as the number of instructions the output of which should be
  taken into account (reduction of the number of states to explore).  As
  expected, the best results are obtained for programs with very simple control
  flow, namely \verb|fdct.c| and \verb|jfdctint.c|, whereas the worst results
  are obtained for programs with nested control statements and procedure calls ,
  namely \verb|ndes.c| and \verb|adpcm.c|. However, let us underline that the
  structure of the source code is not always the dominant factor.  For some
  programs, it appears that the compiler (version and/or optimization) has more
  impact on the capacity of the program slicer to abstract the binary. Example
  of such programs are \verb|expint.c| and \verb|fir.c|.

  %% \todo{AM}{Jette un oeil à ces progs. pour voir si tu trouves une
  %% explication}.

   

  
  %\begin{table}
    %\centering
    %\resizebox{.8\columnwidth}{!}{%
    %\input{fig/results}%
    %}
    %\caption{\centering Results for Mälardalen benchmark. \\
      %Results in each column are expressed as ``-$\gamma$\%~($\alpha$)''% where \\
%%      $\alpha$ is the number of explicitly and implicitly used registers in the original program ;\\
%%      $\beta$ is the number of explicitly and implicitly used registers in the slice ; \\
%%      $\gamma = (\alpha - \beta) / \alpha \times 100$, the gain in percentage.
      %\\\todo{AM}{Réexporter les données}}
    %\label{tab:results}
  %\end{table}

  
  %Overall, the average reduction is $41\%$ with a standard deviation of $24$.
  %This result \todo{AM}{Comparaison avec ACSD 13 ?}
  %It confirms the interest of the approach.
  %The average reduction is $35\%$ for \gcc\ and $55\%$ for \cosmic{}.
  %The reason is that \cosmic\ uses extensively the general purpose registers:
  %the mean value of $\alpha$ is $32$ in the case of \cosmic\ and $20$ for
  %\gcc\ ($14$ for level \texttt{O0}).
  
  %% It is thus expected that the development of the stack frame analysis will
  %% improve the results in the case of \gcc{}. \todo{AM}{Est-ce certain ?
  %% d'après tes notes, il y a moins de load /store dans gcc optim quand dans
  %% cosmic, néanmoins il y a plus de registre utilisés dans cosmic : il faut
  %% creuser cela}.

  %The worst results are obtained with \verb|ndes.c| compiled with
  %\gcc\ (\verb|-O3|) and \verb|adpcm.c| compiled with \gcc\ (\verb|-O2|). While,
  %the binaries are constituted respectively of 886 and 1094 instructions, the
  %slices are constituted of 522 and 421 instructions, i.e. 62\% and 38\% of the
  %instructions of the original program. \verb|ndes.c| and \verb|adpcm.c| do
  %extensive use of nested control statements (if-then-else statements, for-loop
  %and while-loops) and extensive use of calls to the same small procedures.

  %The best results are obtained with \verb|fdct.c| and and \verb|jfdctint.c|
  %both compiled with \cosmic\. While, the binaries are constituted respectively
  %of 218 and 205 instructions, the slices are constituted of 6 and 9
  %instructions, i.e. 3\% and 4\% of the instructions of the original program.
  %\verb|fdct.c| and \verb|jfdctint.c| are two implementations of the same
  %algorithm. They are made of only two not-nested for-loops with extensive
  %sequential computation in both loops and no procedure call.
  
  %\todo{AM}{On se pose la question de la dépendance des résultats au programme. Pour visualiser cela simplement, on pourrait tracer la courbe de gamma en fonction du programme par compilateur (dans le papier) et par niveau d'optimisation et commenter (sur le site web uniquement).}
  

  %Not optimized binaries produced with \gcc\ does extensive use of the stack
  %with a mean of 174 loads or stores per binary. Lowering the number of memory
  %access is one of the major optimization introduced with the first optimization
  %level on \gcc\ (\verb|-O1|) with a mean of 56 loads or stores per binary.
  %Whereas, not optimized binaries produces with \cosmic\ (\verb|-no|) don't do
  %such an extensive use of the stack with a mean of 89 loads or stores per
  %binary. Lowering the number of memory access is not a major optimization with
  %the default and only optimization level on \cosmic\ with a mean of 79 loads or
  %stores per binary.

  %However, the \cosmic\ compiler does an extensive use of the general purpose
  %registers. Indeed, the mean value of $\alpha$ on \cosmic\ is 32 for not optimized
  %binaries and 32 for the default optimization level while the mean value of
  %$\alpha$ for \gcc\ is 14 for not optimized binaries and respectively 20, 18
  %and 20 for the first, second and third optimization level.

  %In fact, unoptimized binaries produced with \cosmic\ are pretty similar to
  %binaries produced by \gcc\ with the optimization level \verb|-O1|.
  %optimizations techniques embedded in the \cosmic\ compiler consist mainly of
  %instruction number reduction through constant propagation techniques. CFG for
  %two binaries from the same source file compiled with different optimization
  %levels with \cosmic\ are mostly identical while they are pretty different
  %while compiled with two different optimization levels.

  %% Pourquoi les résultats sont plutôt interessant :
  
  %\todo{}{je n'ai pas de recul/je ne maitrise pas ce que je dis}\\
  %\todo{JLB}{Clairement c'est faux :-) on ne peut pas dire ça}
  %Embedded code are, for a large part, made of regulation algebra handling
  %vectors and matrices. In this kind of programs most of the computations does
  %not affect the flow of control of the program. So, considering this kind of
  %programs we can state that our approach has a good potential to yield
  %significant state-pace reductions.

  %However, when it comes to slicing programs like services form a real-time
  %operating system, the computations does strongly affect the flow of control of
  %the program and so the gain may be really low.

